C <- rbinom(n, 1, prob = C.pr)
summary(glm(B ~ A, family = binomial))
summary(glm(B[C == 1] ~ A[C == 1], family = binomial))
## There is bias in the intercept, not in the slope.
## Scenario 6. ##
## Continuous covariate, binary outcome, logistic collider.
set.seed(1005)
n <- 1000000
A <- rnorm(n)
B.pr <- exp(0 + 0.1 * A) / (1 + exp(0 + 0.1 * A))
B <- rbinom(n, 1, prob = B.pr)
C.pr <- exp(-1 + 0.1 * A + 0.3 * B) / (1 + exp(-1 + 0.1 * A + 0.3 * B))
C <- rbinom(n, 1, prob = C.pr)
summary(glm(B ~ A, family = binomial))
summary(glm(B[C == 1] ~ A[C == 1], family = binomial))
## There is bias in the intercept, not in the slope.
## Scenario 6. ##
## Continuous covariate, binary outcome, logistic collider.
set.seed(1006)
n <- 1000000
A <- rnorm(n)
B.pr <- exp(0 + 0.1 * A) / (1 + exp(0 + 0.1 * A))
B <- rbinom(n, 1, prob = B.pr)
C.pr <- exp(-1 + 0.1 * A + 0.3 * B) / (1 + exp(-1 + 0.1 * A + 0.3 * B))
C <- rbinom(n, 1, prob = C.pr)
summary(glm(B ~ A, family = binomial))
summary(glm(B[C == 1] ~ A[C == 1], family = binomial))
## There is bias in the intercept, not in the slope.
## Binary covariate, continuous outcome, log-binomial collider.
set.seed(1013)
n <- 1000000
A <- rbinom(n, 1, prob = 0.5)
B <- 0 + 1 * A + rnorm(n)
C.pr <- exp(-1 + 0.4 * A + 0.2 * B)
C <- rbinom(n, 1, prob = C.pr)
summary(glm(B ~ A, family = binomial))
summary(glm(B[C == 1] ~ A[C == 1], family = binomial))
## There is bias in the intercept, not in the slope.
min(C.pr)
max(C.pr)
8*7*6*5/1*2*3*4
8*7*6*5/(1*2*3*4)
2336.44 * 12
3311.58 * 12
exp(0.5) / (1 + exp(0.5))
58/372
65/452
32+29+39+41+32+45+46+68+57+45+52+59
60+65+40+43+46+36+21+43+54+50
58/458
65/545
n <- 10000
Z <- rnorm(n, 0, 1)
X <- rnorm(n, 0, 1)
Y <- 1 + 0.3 * X + rnorm(n, 0, 1)
R.probs <- expit(0.5 + 0.2 * X + 0.2 * Y)
R <- rbinom(n, 1, R.probs)
## More auxiliary functions.
logit <- function (x) log(x / (1 - x))
expit <- function (x) {
res <- exp(x) / (1 + exp(x))
res[x > 0] <- 1 / (1 + exp(- x[x > 0]))   ## This makes the function stable for x > 710.
res
}
set.seed(876)
n <- 10000
Z <- rnorm(n, 0, 1)
X <- rnorm(n, 0, 1)
Y <- 1 + 0.3 * X + rnorm(n, 0, 1)
R.probs <- expit(0.5 + 0.2 * X + 0.2 * Y)
R <- rbinom(n, 1, R.probs)
theta <- rep(0, 7)
theta <- rnorm(7)
theta
options(digits = 4)
if (is.vector(X)) d <- 1 else d <- ncol(X)
psi <- theta[1:(d + 1)]
eta <- theta[(d + 2):(2 * d + 2)]
alpha <- theta[(2 * d + 3):(3 * d + 4)]
Wh <- as.vector(cbind(1, X) %*% eta)
Va <- as.vector(cbind(1, X, Z) %*% alpha)
Wp <- as.vector(cbind(1, X) %*% psi)
r.probs <- (1 - expit2(Wp)) * expit2(Va) + expit2(Wp) * expit2(Va + Wh)
r.probs <- (1 - expit(Wp)) * expit2(Va) + expit(Wp) * expit(Va + Wh)
r.probs <- (1 - expit(Wp)) * expit(Va) + expit(Wp) * expit(Va + Wh)
## Compute the log-likelihood.
y.logit <- Wp + Wh - log( (exp(Va + Wh) + 1) / (1 + exp(Va)) )
y.log <- y.logit - log(1 + exp(y.logit))
y.nlog <- - log(1 + exp(y.logit))
sum(R * Y * y.log, na.rm = TRUE) + sum(R * (1 - Y) * y.nlog, na.rm = TRUE) + sum(R * log(r.probs)) + sum((1 - R) * log(1 - r.probs))
full.logit.lik(theta = theta. X=X, Z=Z, Y=Y, R=R)
full.logit.lik(theta = theta, X=X, Z=Z, Y=Y, R=R)
## IV-TTW - logistic case - full likelihood.
full.logit.lik <- function (theta, X, Z, R, Y) {
## Get the number of covariates, split the parameter vector.
if (is.vector(X)) d <- 1 else d <- ncol(X)
psi <- theta[1:(d + 1)]
eta <- theta[(d + 2):(2 * d + 2)]
alpha <- theta[(2 * d + 3):(3 * d + 4)]
## Compute basic quantities.
Wh <- as.vector(cbind(1, X) %*% eta)
Va <- as.vector(cbind(1, X, Z) %*% alpha)
Wp <- as.vector(cbind(1, X) %*% psi)
## Estimate missingness probabilities.
r.probs <- (1 - expit(Wp)) * expit(Va) + expit(Wp) * expit(Va + Wh)
## Compute the log-likelihood.
y.logit <- Wp + Wh - log( (exp(Va + Wh) + 1) / (1 + exp(Va)) )
y.log <- y.logit - log(1 + exp(y.logit))
y.nlog <- - log(1 + exp(y.logit))
sum(R * Y * y.log, na.rm = TRUE) + sum(R * (1 - Y) * y.nlog, na.rm = TRUE) + sum(R * log(r.probs)) + sum((1 - R) * log(1 - r.probs))
}
full.logit.lik(theta = theta, X=X, Z=Z, Y=Y, R=R)
C <- NA
prm <- NA
if(is.vector(X)) {
if (length(X) != length(Y) | length(X) != length(Z)) stop("Variable lengths differ.")
if (!(is.na(C)) & length(X) != dim(C)[1]) stop("Variable lengths differ.")   ## Must modify this to account for C vector.
} else if (is.matrix(X)) {
if (dim(X)[1] != length(Y) | dim(X)[1] != length(Z)) stop("Variable lengths differ.")
if (!(is.na(C)) & dim(X)[1] != dim(C)[1]) stop("Variable lengths differ.")   ## Again must modify.
} else {
stop("X must be a vector or matrix.")
}
## Diagnostics and setup.
if(is.vector(X)) {
if (length(X) != length(Y) | length(X) != length(Z)) stop("Variable lengths differ.")
if (!(is.na(C))) {
if (is.vector(C)) {
if (length(X) != length(C)) stop("Variable lengths differ.")
} else  if (is.matrix(C)) {
if (length(X) != dim(C)[1]) stop("Variable lengths differ.")
} else {
stop("C must be a vector or matrix.")
}
}
} else if (is.matrix(X)) {
if (dim(X)[1] != length(Y) | dim(X)[1] != length(Z)) stop("Variable lengths differ.")
if (!(is.na(C))) {
if (is.vector(C)) {
if (dim(X)[1] != length(C)) stop("Variable lengths differ.")
} else  if (is.matrix(C)) {
if (dim(X)[1] != dim(C)[1]) stop("Variable lengths differ.")
} else {
stop("C must be a vector or matrix.")
}
}
} else {
stop("X must be a vector or matrix.")
}
## Set up starting values.
if (is.vector(X)) d1 <- 1 else d1 <- ncol(X)
if (is.na(C)) d2 <- 0 else if (is.vector(C)) d2 <- 1 else d2 <- ncol(C)
if (!(is.na(prm)) & length(prm) != 3 * (d1 + d2 + 4)) stop("Starting parameter vector has the wrong length.")
if (is.na(prm)) prm <- rep(0, 3 * (d1 + d2) + 4)
d1
d2
prm
prm <- theta
if (!(is.na(prm)) & length(prm) != 3 * (d1 + d2) + 4) stop("Starting parameter vector has the wrong length.")
if (is.na(prm)) prm <- rep(0, 3 * (d1 + d2) + 4)
if (!(is.na(prm))) if (length(prm) != 3 * (d1 + d2) + 4) stop("Starting parameter vector has the wrong length.")
if (is.na(prm)) prm <- rep(0, 3 * (d1 + d2) + 4)
length(prm)
3 * (d1 + d2) + 4
is.na(prm)
all(is.na(prm))
prm <- NA
is.na(prm)
all(is.na(prm))
if (!(all(is.na(prm)))) if (length(prm) != 3 * (d1 + d2) + 4) stop("Starting parameter vector has the wrong length.")
if (all(is.na(prm))) prm <- rep(0, 3 * (d1 + d2) + 4)
prm
prm <- theta
if (!(all(is.na(prm)))) if (length(prm) != 3 * (d1 + d2) + 4) stop("Starting parameter vector has the wrong length.")
if (all(is.na(prm))) prm <- rep(0, 3 * (d1 + d2) + 4)
## Set up the missingness indicator.
if (sum(is.na(Y)) == 0) stop("The outcome does not contain missing values.")
R <- which(is.na(Y))
Y[1:20]
set.seed(877)
n <- 10000
Z <- rnorm(n, 0, 1)
X <- rnorm(n, 0, 1)
Y <- expit(1 + 0.3 * X)
R.probs <- expit(0.5 + 0.2 * X + 0.2 * Y)
R <- rbinom(n, 1, R.probs)
Y[R == 0] <- NA
## IV-TTW - logistic case - full likelihood.
full.logit.lik <- function (theta, X, Z, R, Y) {
## Get the number of covariates, split the parameter vector.
if (is.vector(X)) d <- 1 else d <- ncol(X)
psi <- theta[1:(d + 1)]
eta <- theta[(d + 2):(2 * d + 2)]
alpha <- theta[(2 * d + 3):(3 * d + 4)]
## Compute basic quantities.
Wh <- as.vector(cbind(1, X) %*% eta)
Va <- as.vector(cbind(1, X, Z) %*% alpha)
Wp <- as.vector(cbind(1, X) %*% psi)
## Estimate missingness probabilities.
r.probs <- (1 - expit(Wp)) * expit(Va) + expit(Wp) * expit(Va + Wh)
## Compute the log-likelihood.
y.logit <- Wp + Wh - log( (exp(Va + Wh) + 1) / (1 + exp(Va)) )
y.log <- y.logit - log(1 + exp(y.logit))
y.nlog <- - log(1 + exp(y.logit))
sum(R * Y * y.log, na.rm = TRUE) + sum(R * (1 - Y) * y.nlog, na.rm = TRUE) + sum(R * log(r.probs)) + sum((1 - R) * log(1 - r.probs))
}
## More auxiliary functions.
logit <- function (x) log(x / (1 - x))
expit <- function (x) {
res <- exp(x) / (1 + exp(x))
res[x > 0] <- 1 / (1 + exp(- x[x > 0]))   ## This makes the function stable for x > 710.
res
}
if (is.vector(X)) d <- 1 else d <- ncol(X)
psi <- theta[1:(d + 1)]
eta <- theta[(d + 2):(2 * d + 2)]
alpha <- theta[(2 * d + 3):(3 * d + 4)]
## Compute basic quantities.
Wh <- as.vector(cbind(1, X) %*% eta)
Va <- as.vector(cbind(1, X, Z) %*% alpha)
Wp <- as.vector(cbind(1, X) %*% psi)
## Estimate missingness probabilities.
r.probs <- (1 - expit(Wp)) * expit(Va) + expit(Wp) * expit(Va + Wh)
## Compute the log-likelihood.
y.logit <- Wp + Wh - log( (exp(Va + Wh) + 1) / (1 + exp(Va)) )
y.log <- y.logit - log(1 + exp(y.logit))
y.nlog <- - log(1 + exp(y.logit))
sum(R * Y * y.log, na.rm = TRUE) + sum(R * (1 - Y) * y.nlog, na.rm = TRUE) + sum(R * log(r.probs)) + sum((1 - R) * log(1 - r.probs))
theta
full.logit.lik(theta, X, Z, R, Y)
prm <- theta
C <- NA
## Diagnostics and setup.
if(is.vector(X)) {
if (length(X) != length(Y) | length(X) != length(Z)) stop("Variable lengths differ.")
if (!(is.na(C))) {
if (is.vector(C)) {
if (length(X) != length(C)) stop("Variable lengths differ.")
} else  if (is.matrix(C)) {
if (length(X) != dim(C)[1]) stop("Variable lengths differ.")
} else {
stop("C must be a vector or matrix.")
}
}
} else if (is.matrix(X)) {
if (dim(X)[1] != length(Y) | dim(X)[1] != length(Z)) stop("Variable lengths differ.")
if (!(is.na(C))) {
if (is.vector(C)) {
if (dim(X)[1] != length(C)) stop("Variable lengths differ.")
} else  if (is.matrix(C)) {
if (dim(X)[1] != dim(C)[1]) stop("Variable lengths differ.")
} else {
stop("C must be a vector or matrix.")
}
}
} else {
stop("X must be a vector or matrix.")
}
## Set up starting values.
if (is.vector(X)) d1 <- 1 else d1 <- ncol(X)
if (is.na(C)) d2 <- 0 else if (is.vector(C)) d2 <- 1 else d2 <- ncol(C)
if (!(all(is.na(prm)))) if (length(prm) != 3 * (d1 + d2) + 4) stop("Starting parameter vector has the wrong length.")
if (all(is.na(prm))) prm <- rep(0, 3 * (d1 + d2) + 4)
d1
d2
prm
## Set up the missingness indicator.
if (sum(is.na(Y)) == 0) stop("The outcome does not contain missing values.")
R <- which(is.na(Y))
R[1:20]
Y[1:20]
## Now run the optimization.
opt <- optim(par = prm, full.logit.lik, method = "BFGS", control = list(fnscale = -1), X = cbind(X, C), R = R, Z = Z, Y = Y, hessian = TRUE)
opt.sd <- sqrt(diag(solve(- opt$hessian)))
means <- opt$par[1:(d1 + d2 + 1)]
sds <- opt.sd[1:(d1 + d2 + 1)]
##
rm(list = ls())
## IV-TTW - logistic case - full likelihood.
full.logit.lik <- function (theta, X, Z, R, Y) {
## Get the number of covariates, split the parameter vector.
if (is.vector(X)) d <- 1 else d <- ncol(X)
psi <- theta[1:(d + 1)]
eta <- theta[(d + 2):(2 * d + 2)]
alpha <- theta[(2 * d + 3):(3 * d + 4)]
## Compute basic quantities.
Wh <- as.vector(cbind(1, X) %*% eta)
Va <- as.vector(cbind(1, X, Z) %*% alpha)
Wp <- as.vector(cbind(1, X) %*% psi)
## Estimate missingness probabilities.
r.probs <- (1 - expit(Wp)) * expit(Va) + expit(Wp) * expit(Va + Wh)
## Compute the log-likelihood.
y.logit <- Wp + Wh - log( (exp(Va + Wh) + 1) / (1 + exp(Va)) )
y.log <- y.logit - log(1 + exp(y.logit))
y.nlog <- - log(1 + exp(y.logit))
sum(R * Y * y.log, na.rm = TRUE) + sum(R * (1 - Y) * y.nlog, na.rm = TRUE) + sum(R * log(r.probs)) + sum((1 - R) * log(1 - r.probs))
}
## More auxiliary functions.
logit <- function (x) log(x / (1 - x))
expit <- function (x) {
res <- exp(x) / (1 + exp(x))
res[x > 0] <- 1 / (1 + exp(- x[x > 0]))   ## This makes the function stable for x > 710.
res
}
set.seed(877)
n <- 10000
Z <- rnorm(n, 0, 1)
X <- rnorm(n, 0, 1)
Y <- expit(1 + 0.3 * X)
R.probs <- expit(0.5 + 0.2 * X + 0.2 * Y)
R <- rbinom(n, 1, R.probs)
Y[R == 0] <- NA
theta <- runif(7)
## Get the number of covariates, split the parameter vector.
if (is.vector(X)) d <- 1 else d <- ncol(X)
psi <- theta[1:(d + 1)]
eta <- theta[(d + 2):(2 * d + 2)]
alpha <- theta[(2 * d + 3):(3 * d + 4)]
## Compute basic quantities.
Wh <- as.vector(cbind(1, X) %*% eta)
Va <- as.vector(cbind(1, X, Z) %*% alpha)
Wp <- as.vector(cbind(1, X) %*% psi)
## Estimate missingness probabilities.
r.probs <- (1 - expit(Wp)) * expit(Va) + expit(Wp) * expit(Va + Wh)
## Compute the log-likelihood.
y.logit <- Wp + Wh - log( (exp(Va + Wh) + 1) / (1 + exp(Va)) )
y.log <- y.logit - log(1 + exp(y.logit))
y.nlog <- - log(1 + exp(y.logit))
sum(R * Y * y.log, na.rm = TRUE) + sum(R * (1 - Y) * y.nlog, na.rm = TRUE) + sum(R * log(r.probs)) + sum((1 - R) * log(1 - r.probs))
full.logit.lik(theta=theta, X=X, Z=Z, Y=Y, R=R)
C <- NA
prm <- theta
## Diagnostics and setup.
if(is.vector(X)) {
if (length(X) != length(Y) | length(X) != length(Z)) stop("Variable lengths differ.")
if (!(is.na(C))) {
if (is.vector(C)) {
if (length(X) != length(C)) stop("Variable lengths differ.")
} else  if (is.matrix(C)) {
if (length(X) != dim(C)[1]) stop("Variable lengths differ.")
} else {
stop("C must be a vector or matrix.")
}
}
} else if (is.matrix(X)) {
if (dim(X)[1] != length(Y) | dim(X)[1] != length(Z)) stop("Variable lengths differ.")
if (!(is.na(C))) {
if (is.vector(C)) {
if (dim(X)[1] != length(C)) stop("Variable lengths differ.")
} else  if (is.matrix(C)) {
if (dim(X)[1] != dim(C)[1]) stop("Variable lengths differ.")
} else {
stop("C must be a vector or matrix.")
}
}
} else {
stop("X must be a vector or matrix.")
}
## Set up starting values.
if (is.vector(X)) d1 <- 1 else d1 <- ncol(X)
if (is.na(C)) d2 <- 0 else if (is.vector(C)) d2 <- 1 else d2 <- ncol(C)
if (!(all(is.na(prm)))) if (length(prm) != 3 * (d1 + d2) + 4) stop("Starting parameter vector has the wrong length.")
if (all(is.na(prm))) prm <- rep(0, 3 * (d1 + d2) + 4)
## Set up the missingness indicator.
if (sum(is.na(Y)) == 0) stop("The outcome does not contain missing values.")
R0 <- which(is.na(Y))
opt <- optim(par = prm, full.logit.lik, method = "BFGS", control = list(fnscale = -1), X = cbind(X, C), R = R0, Z = Z, Y = Y, hessian = TRUE)
## Will need to correct that.
?optim
exp(1) / (1 + exp(1))
n <- 10000
X <- rbinom(n, 1, 0.5)
expit <- function (x) exp(x) / (1 + exp(x))
Y.prob <- expit(0.5 + 0.2 * X)
Y <- rbinom(n, 1, prob = Y.prob)
R.prob <- expit(0.3 * X + 0.4 * Y + 0.5 * X * Y)
R <- rbinom(n, 1, prob = R.prob)
summary(glm(Y ~ X, family = binomial))
summary(glm(Y[R == 1] ~ X[R == 1], family = binomial))
## Generate data.
set.seed(5258)
n <- 10000
X <- rbinom(n, 1, 0.5)
expit <- function (x) exp(x) / (1 + exp(x))
Y.prob <- expit(0.5 + 0.2 * X)
Y <- rbinom(n, 1, prob = Y.prob)
R.prob <- expit(0.3 * X + 0.4 * Y + 0.5 * X * Y)
R <- rbinom(n, 1, prob = R.prob)
## Odds ratios (true value = 0.2).
summary(glm(Y ~ X, family = binomial))   ##
summary(glm(Y[R == 1] ~ X[R == 1], family = binomial))
x <- as.factor(c(3, 3, 6, 2, 7, 4, 3, 6, 5))    # Create example data
x
x^2
x * 2
as.numeric(x) * 2
as.numeric(x)
MCtest <- function (f1, f2, N1, N2, seed = NULL, abs = FALSE, mc.iter = 1000) {
## Set up.
if (!(is.null(seed))) set.seed(seed)
P <- length(f1)
## Compute the test statistic for observed data.
if (abs == TRUE) test.stat <- sum(abs(f1 - f2)) else test.stat <- sum((f1 - f2)^2)
## Store Monte Carlo test statistics here.
mc.stats <- rep(0, mc.iter)
## Loop over MC iterations, generate data, compute test statistics.
for (i in 1:mc.iter) {
f2mc <- rep(0, P)
for (j in 1:P) {
g <- rbinom(N2, 2, f1[j])
f2mc[j] <- mean(g) / 2
}
if (abs == TRUE) mc.stats[i] <- sum(abs(f1 - f2mc)) else mc.stats[i] <- sum((f1 - f2mc)^2)
}
## Sort MC test statistics, get the MC p-value, return.
mc.stats <- sort(mc.stats)
pval <- sum(mc.stats >= test.stat) / mc.iter
return(list("test.stat" = test.stat, "mc.stats" = mc.stats, "Pvalue" = pval))
}
## Set it up.
set.seed(4444)
N1 <- 20000
N2 <- 10000
P <- 20
true.eafs <- runif(P, 0.1, 0.9)
## Generate data, for sample 1.
G1 <- matrix(0, N1, P)
for (i in 1:P) G1[, i] <- rbinom(N1, 2, true.eafs[i])
f1est <- colMeans(G1) / 2
## Exposure/outcome values don't matter here.
## Generate data for sample 2.
G2 <- matrix(0, N2, P)
for (i in 1:P) G2[, i] <- rbinom(N2, 2, true.eafs[i])
f2est <- colMeans(G2) / 2
## Run the test (takes about 12 seconds).
system.time(test1 <- MCtest(f1est, f2est, N1, N2, seed = 123))
test1$Pvalue
## Does not reject - P-value = 0.765.
## Likewise with absolute differences.
system.time(test2 <- MCtest(f1est, f2est, N1, N2, seed = 123, abs = TRUE))
test2$Pvalue
## Does not reject either.
## Plot it.
hist(test2$mc.stats, xlim = c(0, 1.1 * max(test2$test.stat, test2$mc.stats)), main = "Histogram", xlab = "Test Statistic", ylab = "Number of MC Samples")
abline(v = test2$test.stat, col = "red")
## The scale of the test statistic does not matter.
## Set it up.
set.seed(5555)
N1 <- 20000
N2 <- 10000
P <- 20
## Set different eafs for the two populations.
true.eafs1 <- runif(P, 0.1, 0.9)
true.eafs2 <- true.eafs1 + 0.01
## Generate data, for sample 1.
G1 <- matrix(0, N1, P)
for (i in 1:P) G1[, i] <- rbinom(N1, 2, true.eafs1[i])
f1est <- colMeans(G1) / 2
## Generate data for sample 2.
G2 <- matrix(0, N2, P)
for (i in 1:P) G2[, i] <- rbinom(N2, 2, true.eafs2[i])
f2est <- colMeans(G2) / 2
## Run the test.
system.time(test1 <- MCtest(f1est, f2est, N1, N2, seed = 456))
test1$Pvalue
system.time(test2 <- MCtest(f1est, f2est, N1, N2, seed = 456, abs = TRUE))
test2$Pvalue
## Both tests easily reject - Pvalue = 0
## Plot it.
hist(test2$mc.stats, xlim = c(0, 1.1 * max(test2$test.stat, test2$mc.stats)), main = "Histogram", xlab = "Test Statistic", ylab = "Number of MC Samples")
abline(v = test2$test.stat, col = "red")
## CASE 3: POPULATION SUBSTRUCTURE
## 80% of individuals in Sample 2 come from the same
## population as Sample 1 but 20% are different.
## Set it up.
set.seed(6666)
N1 <- 20000
N2 <- 10000
P <- 20
## Set different eafs for the two population groups.
true.eafs1 <- runif(P, 0.1, 0.9)
true.eafs2 <- true.eafs1 + 0.01
## Generate data, for sample 1.
G1 <- matrix(0, N1, P)
for (i in 1:P) G1[, i] <- rbinom(N1, 2, true.eafs1[i])
f1est <- colMeans(G1) / 2
## Generate data for sample 2.
G2 <- matrix(0, N2, P)
for (i in 1:P) G2[, i] <- c(rbinom(0.8 * N2, 2, true.eafs1[i]), rbinom(0.2 * N2, 2, true.eafs2[i]))
f2est <- colMeans(G2) / 2
## Run the test.
system.time(test1 <- MCtest(f1est, f2est, N1, N2, seed = 789))
test1$Pvalue
system.time(test2 <- MCtest(f1est, f2est, N1, N2, seed = 789, abs = TRUE))
test2$Pvalue
## Both tests reject once again.
## Plot it.
hist(test2$mc.stats, xlim = c(0, 1.1 * max(test2$test.stat, test2$mc.stats)), main = "Histogram", xlab = "Test Statistic", ylab = "Number of MC Samples")
abline(v = test2$test.stat, col = "red")
1.125^7
install.packages(c("BH", "BiocManager", "broom", "broom.helpers", "car", "caret", "checkmate", "cli", "clipr", "cluster", "colorspace", "conquer", "crayon", "data.table", "DEoptimR", "desc", "digest", "dplyr", "e1071", "evaluate", "fansi", "foreign", "formatR", "future", "future.apply", "gert", "ggplot2", "glmnet", "globals", "glue", "gmp", "gt", "gtsummary", "haven", "Hmisc", "httr", "ipred", "jsonlite", "knitr", "labelled", "lattice", "lme4", "lmtest", "magrittr", "maptools", "MASS", "Matrix", "matrixStats", "maxLik", "MendelianRandomization", "mgcv", "multcomp", "mvtnorm", "nlme", "nloptr", "openssl", "openxlsx", "parallelly", "plyr", "polspline", "processx", "progressr", "proxy", "ps", "quantreg", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "RCurl", "readxl", "recipes", "restfulr", "rjson", "rlang", "rmarkdown", "rms", "robustbase", "roxygen2", "rprojroot", "sass", "scales", "sp", "stringi", "survival", "testthat", "TH.data", "tibble", "tidyselect", "tinytex", "tzdb", "usethis", "utf8", "vctrs", "VGAM", "waldo", "withr", "xfun", "XML", "yaml", "zoo"))
library(MendelianRandomization)
install.packages(c("BH", "RcppArmadillo", "sandwich", "vctrs"))
install.packages(c("BH", "car", "RcppArmadillo", "vctrs"))
library(MendelianRandomization)
update.packages(vctrs)
update.packages("vctrs")
library(MendelianRandomization)
update.packages("vctrs")
library(MendelianRandomization)
install.packages(c("BH", "RcppArmadillo", "vctrs"))
setwd("C:/Users/bj20642/OneDrive - University of Bristol/Desktop/IVsel GitHub/Regression Analyses")
